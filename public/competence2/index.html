<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="robots" content="index, follow">
        
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://but.mougammadaly.fr/rss.xml">
        
        <title>Jessy MOUGAMMADALY - BUT 3 | Compétence 2 - Optimiser des applications</title>

        <link rel="preload" href="https://but.mougammadaly.fr/css/style.css" as="style">
        <link rel="stylesheet" href="https://raw.githack.com/Speyll/suCSS/main/reset-min.css" crossorigin="anonymous">
        <link rel="stylesheet" href="https://raw.githack.com/Speyll/suCSS/main/suCSS-min.css" crossorigin="anonymous">
        <link rel="stylesheet" href="https://but.mougammadaly.fr/css/style.css">
        <link rel="stylesheet" href="https://but.mougammadaly.fr/css/custom.css">

        <!-- Add favicon with appropriate sizes -->
        <link rel="icon" href="https:&#x2F;&#x2F;but.mougammadaly.fr/favicon.ico">
        
    </head>
    <body>
        
        
        
        
        <nav id="nav-bar">
            
            <a href="&#x2F;" class="">
                
                &#x2F;home&#x2F;
            </a>
            
            <a href="&#x2F;competence1" class="">
                
                &#x2F;competence1&#x2F;
            </a>
            
            <a href="&#x2F;competence2" class="">
                
                &#x2F;competence2&#x2F;
            </a>
            
            <a href="&#x2F;competence6" class="">
                
                &#x2F;competence6&#x2F;
            </a>
            

            <div class="theme-toggle" id="theme-toggle" role="button" tabindex="0" aria-label="Toggle theme"
                data-icon-base="https://but.mougammadaly.fr/icons.svg"
                data-icon-dark="#darkMode"
                data-icon-light="#lightMode"
                data-sound-src="https://but.mougammadaly.fr/click.ogg">
                <svg class="icon">
                    <use id="theme-icon"></use>
                </svg>
            </div>
        </nav>

        <main>
            
<article class="post">
    <header class="post-header">
        

        

        <h1>Compétence 2 - Optimiser des applications</h1>
    </header>



    <div class="post-content">
        <p>Cette section est dédiée à la présentation du travail réalisé correspondant à la compétence 2 lors de l'alternance à l'ESTA.</p>
<h2 id="presentation-de-la-competence-et-des-apprentissages-critiques">Présentation de la compétence et des apprentissages critiques</h2>
<p>Cette compétence se centre sur l'optimisation des applications</p>
<table><thead><tr><th>Composantes</th><th>AC 1</th><th>AC 2</th><th>AC 3</th><th>AC 4</th></tr></thead><tbody>
<tr><td>Appréhender et construire des algorithmes</td><td>Analyser un problème avec méthode</td><td>Comparer des algorithmes pour des problèmes classiques</td><td>Formaliser et mettre en oeuvre des outils mathématiques pour l'informatique</td><td></td></tr>
<tr><td>Sélectionner les algorithmes adéquats pour répondre à un problème donné</td><td>Choisir des structures de données complexes adaptées au problème</td><td>Utiliser des techniques algorithmiques adaptées pour des problèmes complexes</td><td>Comprendre les enjeux et moyens de sécurisation des données et du code</td><td>Évaluer l'impact environnemental et sociétal des solution proposées</td></tr>
<tr><td>Analyser et optimiser des applications</td><td>Anticiper les résultats de divers métriques</td><td>Profiler, analyser et justifier le comportement d'un code existant</td><td>Choisir et utiliser des bibliothèques et méthodes dédiées au domaine d'application</td><td></td></tr>
</tbody></table>
<h2 id="diviser-pour-mieux-regner-des-workers-distribues">Diviser pour mieux régner : des workers distribués</h2>
<p>Pour commencer, si vous voulez un descriptif du projet dans son ensemble, je vous invite à aller voir la section dédiée à la
<a href="/competence1/">compétence 1</a>. Dans cette section, nous allons aborder les aspects liés à l'optimisation que ce soit d'un point de vue
métrique, ou d'un point de vue algorithme.</p>
<p>Comme l'indique le titre de cette partie, nous allons commencer par parler des workers distribués. Déjà, une première question peut se
poser : "Pourquoi faire des workers et pourquoi les distribuer ?". Les raisons sont plutôt simples, tout d'abord, séparer les calculs et
les analyses des pages de l'application principale permet de diminuer la charge sur celle-ci, les serveurs loués par l'ESTA ne permettant
pas une grande charge de calcul, il est important de pouvoir distribuer cette charge ailleurs, d'où la mise en place de workers séparés.</p>
<p>Maintenant, pour répondre à la question de l'aspect distribué de ces workers, je dois d'abord vous expliquer que comme dit précédemment,
l'ESTA avait des contraintes matérielles assez importantes, il n'était pas possible de faire tourner beaucoup d'analyses en parallèle et
la charge était si importante que l'ancien outil (une preuve de concept développée en stage) cessait de fonctionner ou n'arrivait plus
à continuer les analyses. Dû à la charge trop importante, il arrivait que l'ancienne application cause des saturations de la mémoire et
fasse planter le serveur hôte.</p>
<p>En prenant en considération tout cela, j'ai décidé de reprendre le développement de l'application précédente de zéro comme mentionné dans la section
dédiée à la <a href="/competence1/">compétence 1</a>.</p>
<p>Une différence majeure entre l'application actuelle et l'application précédente est la technologie principale utilisée. Dans l'application
précédente qui servait de preuve de concept, pour effectuer les tâches d'analyses, j'ai eu recours à JavaScript et Puppeteer pour le
scraping. A l'époque, par rapport aux contraintes de temps liées au stage, je n'avais eu trop le choix de que de me reposer sur les
technologies précédentes pour développer plus rapidement, cependant, le gain en temps de développement s'est répercuté sur la
maintenabilité, la fiabilité, et les performances globales de l'application.</p>
<p>En ayant tout cela en tête, j'ai décidé de reconsidérer toutes les options disponibles et prendre le temps qu'il faut pour obtenir un
résultat satisfaisant pour l'analyse des pages web. Mon choix s'est porté assez rapidement sur le fait de décomposer le backend entre
l'API pour gérer les données et des workers distribués pour gérer la collecte/analyse de données. Les technologies choisies pour les
workers répondent aux limitations matérielles et aux contraintes d'échelle concernant la quantité de pages et de critères à analyser.
La base des workers a été intégralement faite en Rust, pour voir la pile technologique simplifiée des workers je vous invite à regarder
la trace suivante.</p>
<p><img src="https://but.mougammadaly.fr/competence2/stack_composition.png" alt="Pile technologique simplifiée des workers" title="Pile technologique simplifiée des workers" /></p>
<table><thead><tr><th><strong>Trace 1 : Pile technologique simplifiée des workers</strong></th></tr></thead><tbody>
</tbody></table>
<p>On constate qu'en plus de Rust, on voit la présence de Python dans la gestion des tâches des workers, cela est dû à l'utilisation de
l'outil <a href="https://docs.celeryq.dev/en/stable">Celery</a> qui permet de faire fonctionner des workers distribués, de gérer des tâches en
parallèle et de gérer l'ordonnancement de l'ensemble des tâches et de workers sans qu'on n'ait besoin de développer une solution propre.</p>
<p>Pour pouvoir appeler des fonctions Rust en Python, j'ai eu besoin de créer un "binding" entre les fonctions Rust et les fonctions Python.
Cela a permis de garder les aspects positifs de Rust (meilleure gestion de la mémoire et temps d'exécution rapide) et de pouvoir garder un
backend avec un écosystème centré autours de Python (l'API étant faite avec Django et donc Python) et donc de pouvoir utiliser le
programme Rust directement depuis Celery. Celery a permis aussi de centraliser tous les types de tâches à effectuer comme les tâches
utilisant Google Lighthouse et les tâches utilisant Greenframe, Celery va détecter quel outil il doit appeler et va lancer un processus
lié à cet outil à retourner le résultat.</p>
<p>Pour simplifier le développement, mais aussi diminuer fortement l'impact du calcul de certains critères, j'ai décidé d'employer
différentes méthodes pour récupérer les informations relatives à un critère ou pour naviguer vers la page à analyser. Tout
d'abord, pour le scraping dynamique, j'ai utilisé ThirtyFour qui est une implémentation de Selenium en Rust. Ensuite, pour le scraping
statique, j'ai utilisé la bibliothèque "scraper", il n'est pas toujours nécessaire d'avoir toutes les fonctionnalités d'un navigateur
ou d'avoir une navigation dynamique pour le calcul de certains critères. Après, j'ai ajouté la bibliothèque "image" et "imageproc"
permettant la manipulation des images car certains critères nécessitent des opérations sur les aspects visuels des pages, on doit donc
prendre une capture d'écran de la page, puis la manipuler et l'analyser. Les opérations liées aux images sont coûteuses, ce qui a
nécessité une réflexion sur la manière d'optimiser à la fois la mémoire utilisée par rapport à ces opérations, mais aussi la complexité
des algorithmes associés.</p>
<p><img src="https://but.mougammadaly.fr/competence2/architecture_worker_only.png" alt="Architecture distribuée des workers" title="Architecture distribuée des workers" /></p>
<table><thead><tr><th><strong>Trace 2 : Architecture distribuée des workers</strong></th></tr></thead><tbody>
</tbody></table>
<p>Cette trace est tirée de la trace 1 de la compétence 1, mais ici, nous nous focalisons sur le côté distribué des workers. Il est important
de noter que la partie liée à l'API et à l'interface utilisateur a été enlevée. On peut d'abord constater des similarités entre cette
trace et la précédente, notamment, sur la gestion des tâches et leur répartition. Il y a néanmoins des informations supplémentaires qui
nous sont données, dans un premier temps, Google Lighthouse nécessite une console sans interface de Chrome pour fonctionner, alors que
dans un second temps, on constate que les workers Rust utilisent un Chrome Driver (contenu dans un conteneur à part) pour fonctionner, ce
qui amène à une duplication de navigateur. Une duplication qui n'a pas pu être éviter car Lighthouse ne pouvait pas fonctionner avec un
web driver directement. Ensuite, on peut remarquer la présence d'un lien entre Greenframe et le programme Docker de la machine, c'est dû
au fonctionnement de Greenframe qui nécessite l'exécution d'un conteneur contenant un navigateur pour récupérer des données de métriques
concernant l'usage des ressources matérielles pour naviguer sur la page à analyser. Le navigateur utilisée par Greenframe ne peut
malheureusement pas servir pour d'autres tâches, sinon cela pourrait fausser les résultats obtenus par Greenframe concernant l'usage
matériel.</p>
<p>La trace 2 montre aussi la présence d'opération en parallèle (la superposition d'éléments multiples en dessous des workers et le fait
que plusieurs flêches sortent du Worker Pool Manager, le gestionneur des instances de workers), j'ai mentionné plus haut le fait que les
workers ont été pensés pour pouvoir être distribués, l'intérêt est dans un premier temps la diminution de la charge sur les serveurs et
sur l'application principale, mais la capacité de pouvoir changer l'échelle et le nombre de pages analysées en parallèle juste en ajoutant
de nouveaux workers.</p>
<p>Après avoir effectué de nombreuses optimisations en termes de mémoire et d'utilisation du CPU pour les workers, ils sont en capacité de
tourner sur des machines peu performantes avec 4Go de RAM et des processeurs avec une puissance de calcul tout à fait relative. Une des
méthodes d'optimisation principales pour alléger l'impact sur les ressources a été d'employer des spécificités de Rust permettant le fait
de rendre aisément le retour de fonction statique après un premier appel, ce qui fait que les fonctions dépendant d'autres fonctions
utilisent des résultats mis en cache pour éviter de relancer des processus qui peuvent être longs et lourds.</p>
<h2 id="de-l-asynchrone-pour-gagner-en-temps">De l'asynchrone pour gagner en temps</h2>
<p>L'un des objectifs premier du travail de cette année a été de minimiser le temps d'analyse pour rapport, pour ce faire, il fallait d'abord
minimiser le temps de calcul d'une page, puis minimiser le temps pour calculer certains critères quand cela était possible.</p>
<p><img src="https://but.mougammadaly.fr/competence2/worker_description.png" alt="Description de l&#39;algorithme d&#39;un worker pour analyser une page" title="Description de l&#39;algorithme d&#39;un worker pour analyser une page" /></p>
<table><thead><tr><th><strong>Trace 3 : Description de l'algorithme d'un worker pour analyser une page</strong></th></tr></thead><tbody>
</tbody></table>

    </div>

    
</article>

        </main>

        <footer>
            <hr>
<div id="footer-container">
    <p>Made using <a target="_blank" rel="noopener noreferrer" href="https://github.com/Speyll/anemone">anemone</a> Zola theme</p>
</div>

        </footer>

        <!-- Move JS to end of body and add defer -->
        <script src="https://but.mougammadaly.fr/js/script.js" defer></script>
    </body>
</html>
